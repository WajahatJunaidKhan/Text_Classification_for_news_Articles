{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 4243451,
          "sourceType": "datasetVersion",
          "datasetId": 32526
        }
      ],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-29T13:23:23.39033Z",
          "iopub.execute_input": "2024-07-29T13:23:23.391189Z",
          "iopub.status.idle": "2024-07-29T13:23:30.7081Z",
          "shell.execute_reply.started": "2024-07-29T13:23:23.39112Z",
          "shell.execute_reply": "2024-07-29T13:23:30.706998Z"
        },
        "trusted": true,
        "id": "p9LH76iDq19w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_json(\"News Data.json\",lines=True)\n",
        "df.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-29T13:24:50.260804Z",
          "iopub.execute_input": "2024-07-29T13:24:50.26166Z",
          "iopub.status.idle": "2024-07-29T13:24:52.336376Z",
          "shell.execute_reply.started": "2024-07-29T13:24:50.261624Z",
          "shell.execute_reply": "2024-07-29T13:24:52.334953Z"
        },
        "trusted": true,
        "id": "PXqwNn7Qq19x",
        "outputId": "60d4cf6b-306c-40ac-93c7-39b882a7de85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                link  \\\n",
              "0  https://www.huffingtonpost.com/entry/house-gop...   \n",
              "1  https://www.huffingtonpost.com/entry/patrick-m...   \n",
              "2  https://www.huffingtonpost.com/entry/senators-...   \n",
              "3  https://www.huffingtonpost.com/entry/wednesday...   \n",
              "4  https://www.huffingtonpost.com/entry/lenore-an...   \n",
              "5  https://www.huffingtonpost.com/entry/chinese-c...   \n",
              "6  https://www.huffingtonpost.com/entry/lewandows...   \n",
              "7  https://www.huffingtonpost.com/entry/pat-rober...   \n",
              "8  https://www.huffpost.com/entry/rep-kevin-mccar...   \n",
              "9  https://www.huffingtonpost.com/entry/supreme-c...   \n",
              "\n",
              "                                            headline  category  \\\n",
              "0  House GOP Faceplant On Ethics Coup Shows Publi...  POLITICS   \n",
              "1  Rep. Patrick Murphy Comes Out In Favor Of Iran...  POLITICS   \n",
              "2  Senators Strike Funding Deal To Help Flint Rep...  POLITICS   \n",
              "3  Wednesday's Morning Email: Stormy Daniels Suin...  POLITICS   \n",
              "4  Crime Survivors Are Organizing. They Want Crim...  POLITICS   \n",
              "5  Chinese Cyber-Attacks: Will the United States ...  POLITICS   \n",
              "6  Donald Trump’s Top Staff Make Big Bucks But Do...  POLITICS   \n",
              "7  A GOP Senator Told A Democrat To Lighten Up Du...  POLITICS   \n",
              "8  Rep. Kevin McCarthy Deletes Tweet Singling Out...  POLITICS   \n",
              "9  Supreme Court To Hear Dispute Over California ...  POLITICS   \n",
              "\n",
              "                                   short_description  \\\n",
              "0  Public outrage blocked a secret, midnight effo...   \n",
              "1     Many other Florida Democrats remain undecided.   \n",
              "2  The measure was fast-tracked so a vote could c...   \n",
              "3  A porn star is suing the president; let that s...   \n",
              "4  A troubled teen turned prosecutor is bringing ...   \n",
              "5  To avoid escalatory steps, the United States a...   \n",
              "6  Corey Lewandowski has made $541,000 from the c...   \n",
              "7  Other members of his committee were not having...   \n",
              "8  The Republican leader warned that the wealthy ...   \n",
              "9  The “crisis pregnancy centers” counsel women n...   \n",
              "\n",
              "                                             authors       date  \n",
              "0                          Ryan Grim and Matt Fuller 2017-01-03  \n",
              "1                                      Amanda Terkel 2015-08-31  \n",
              "2                                 Laura Barrón-López 2016-02-24  \n",
              "3                                       Eliot Nelson 2018-03-07  \n",
              "4                                        Nico Pitney 2016-08-31  \n",
              "5  Franz-Stefan Gady, ContributorSenior Fellow, E... 2014-05-27  \n",
              "6                                          S.V. Date 2016-10-21  \n",
              "7                                         Sam Levine 2017-01-19  \n",
              "8                                     Mary Papenfuss 2018-10-28  \n",
              "9                           Lawrence Hurley, Reuters 2017-11-13  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1be6273-d35b-4a54-9748-513e14022697\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link</th>\n",
              "      <th>headline</th>\n",
              "      <th>category</th>\n",
              "      <th>short_description</th>\n",
              "      <th>authors</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/house-gop...</td>\n",
              "      <td>House GOP Faceplant On Ethics Coup Shows Publi...</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>Public outrage blocked a secret, midnight effo...</td>\n",
              "      <td>Ryan Grim and Matt Fuller</td>\n",
              "      <td>2017-01-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/patrick-m...</td>\n",
              "      <td>Rep. Patrick Murphy Comes Out In Favor Of Iran...</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>Many other Florida Democrats remain undecided.</td>\n",
              "      <td>Amanda Terkel</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/senators-...</td>\n",
              "      <td>Senators Strike Funding Deal To Help Flint Rep...</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>The measure was fast-tracked so a vote could c...</td>\n",
              "      <td>Laura Barrón-López</td>\n",
              "      <td>2016-02-24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/wednesday...</td>\n",
              "      <td>Wednesday's Morning Email: Stormy Daniels Suin...</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>A porn star is suing the president; let that s...</td>\n",
              "      <td>Eliot Nelson</td>\n",
              "      <td>2018-03-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/lenore-an...</td>\n",
              "      <td>Crime Survivors Are Organizing. They Want Crim...</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>A troubled teen turned prosecutor is bringing ...</td>\n",
              "      <td>Nico Pitney</td>\n",
              "      <td>2016-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/chinese-c...</td>\n",
              "      <td>Chinese Cyber-Attacks: Will the United States ...</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>To avoid escalatory steps, the United States a...</td>\n",
              "      <td>Franz-Stefan Gady, ContributorSenior Fellow, E...</td>\n",
              "      <td>2014-05-27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/lewandows...</td>\n",
              "      <td>Donald Trump’s Top Staff Make Big Bucks But Do...</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>Corey Lewandowski has made $541,000 from the c...</td>\n",
              "      <td>S.V. Date</td>\n",
              "      <td>2016-10-21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/pat-rober...</td>\n",
              "      <td>A GOP Senator Told A Democrat To Lighten Up Du...</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>Other members of his committee were not having...</td>\n",
              "      <td>Sam Levine</td>\n",
              "      <td>2017-01-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>https://www.huffpost.com/entry/rep-kevin-mccar...</td>\n",
              "      <td>Rep. Kevin McCarthy Deletes Tweet Singling Out...</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>The Republican leader warned that the wealthy ...</td>\n",
              "      <td>Mary Papenfuss</td>\n",
              "      <td>2018-10-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/supreme-c...</td>\n",
              "      <td>Supreme Court To Hear Dispute Over California ...</td>\n",
              "      <td>POLITICS</td>\n",
              "      <td>The “crisis pregnancy centers” counsel women n...</td>\n",
              "      <td>Lawrence Hurley, Reuters</td>\n",
              "      <td>2017-11-13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1be6273-d35b-4a54-9748-513e14022697')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1be6273-d35b-4a54-9748-513e14022697 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1be6273-d35b-4a54-9748-513e14022697');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a353a82e-fe76-4308-bbc9-e0e9cc4dabfc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a353a82e-fe76-4308-bbc9-e0e9cc4dabfc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a353a82e-fe76-4308-bbc9-e0e9cc4dabfc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 25403,\n  \"fields\": [\n    {\n      \"column\": \"link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25398,\n        \"samples\": [\n          \"https://www.huffingtonpost.com/entry/jameis-winston-uber-driver-groping-nfl_us_5a0f2c0fe4b0dd63b1aa2388\",\n          \"https://www.huffingtonpost.com/entry/hulu-what-to-watch_us_5adfa2f9e4b061c0bfa28ae5\",\n          \"https://www.huffingtonpost.com/entry/obama-bill-clinton-golf_us_55cf7821e4b07addcb432b09\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"headline\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25314,\n        \"samples\": [\n          \"Man With Parkinson's Takes On 'American Ninja Warrior' Course, Inspires Us All\",\n          \"Hollywood Producer Brett Ratner Sues Woman Who Accused Him Of Rape\",\n          \"Terry Jordan, Mississippi Woman, Sold Wrong Foreclosed Home\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"POLITICS\",\n          \"WORLD NEWS\",\n          \"TECH\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"short_description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 22650,\n        \"samples\": [\n          \"This marks Mauk's third suspension in four months.\",\n          \"\\\"Ascension\\\" is a new mini-series on the Syfy Channel that shows a lot of promise. It is a story about a manned spaceship on a one hundred year journey to a new planet, that is also a murder mystery. Both facets of the show are interesting and so you get the best of both worlds.\",\n          \"It\\u2019s a great time to break free from the tyranny of your mobile operating system. But which one? For Seshu Kiran, it meant\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5037,\n        \"samples\": [\n          \"Damon Beres, Ben Walsh, and Tiara Chiaramonte\",\n          \"Eric Olson, AP\",\n          \"Michele Carroll, ContributorKnown around NYC as Logan's Mom...How did that happen?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2012-01-28 00:00:00\",\n        \"max\": \"2022-09-22 00:00:00\",\n        \"num_unique_values\": 2900,\n        \"samples\": [\n          \"2016-02-04 00:00:00\",\n          \"2015-07-19 00:00:00\",\n          \"2019-01-04 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "source": [
        "# @title category\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df.groupby('category').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "cellView": "form",
        "id": "ktDO1UqkYwn9",
        "outputId": "d6ae38c8-d9ca-4afe-f1d6-dd28bec94fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the stopwords and initialize the stemmer and lemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove punctuations\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and stem or lemmatize the words\n",
        "    words = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    # words = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    # Join the words back into a string\n",
        "    processed_text = ' '.join(words)\n",
        "    return processed_text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-29T13:25:04.623196Z",
          "iopub.execute_input": "2024-07-29T13:25:04.624063Z"
        },
        "trusted": true,
        "id": "effbC7COq19z",
        "outputId": "19c6fb76-97d7-453e-84f5-0f7fd9cbf77a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the 'headline' column\n",
        "df['headline'] = df['headline'].apply(preprocess_text)\n",
        "\n",
        "# Preprocess the 'short_description' column\n",
        "df['short_description'] = df['short_description'].apply(preprocess_text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T18:19:36.602381Z",
          "iopub.execute_input": "2023-03-10T18:19:36.60291Z",
          "iopub.status.idle": "2023-03-10T18:22:15.83134Z",
          "shell.execute_reply.started": "2023-03-10T18:19:36.602858Z",
          "shell.execute_reply": "2023-03-10T18:22:15.830182Z"
        },
        "trusted": true,
        "id": "gHJ3uWhLq190"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_categories = ['POLITICS', 'WORLD NEWS', 'ENTERTAINMENT', 'SPORTS', 'BUSINESS', 'TECH']\n",
        "\n",
        "# Filter out rows with categories not in the top 6\n",
        "df = df[df['category'].isin(top_categories)]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T18:22:27.329729Z",
          "iopub.execute_input": "2023-03-10T18:22:27.330797Z",
          "iopub.status.idle": "2023-03-10T18:22:27.380832Z",
          "shell.execute_reply.started": "2023-03-10T18:22:27.330744Z",
          "shell.execute_reply": "2023-03-10T18:22:27.379734Z"
        },
        "trusted": true,
        "id": "gScN1E_eq191"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T18:22:49.325602Z",
          "iopub.execute_input": "2023-03-10T18:22:49.326151Z",
          "iopub.status.idle": "2023-03-10T18:22:49.335526Z",
          "shell.execute_reply.started": "2023-03-10T18:22:49.326107Z",
          "shell.execute_reply": "2023-03-10T18:22:49.334343Z"
        },
        "trusted": true,
        "id": "FA65Rbgxq191",
        "outputId": "f6f355a9-49b1-4216-fca8-879141f18fce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25403, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(df['headline'] + ' ' + df['short_description'], df['category'], test_size=0.3, random_state=1)\n",
        "\n",
        "# Tokenize the train and test texts\n",
        "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T18:22:54.704144Z",
          "iopub.execute_input": "2023-03-10T18:22:54.704584Z",
          "iopub.status.idle": "2023-03-10T18:23:43.563587Z",
          "shell.execute_reply.started": "2023-03-10T18:22:54.704539Z",
          "shell.execute_reply": "2023-03-10T18:23:43.5625Z"
        },
        "trusted": true,
        "id": "DdEtwq7hq192"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the labels to numerical values\n",
        "labels_map = {label: i for i, label in enumerate(train_labels.unique())}\n",
        "train_labels = [labels_map[label] for label in train_labels.tolist()]\n",
        "test_labels = [labels_map[label] for label in test_labels.tolist()]\n",
        "\n",
        "# Convert the train and test data to PyTorch tensors\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
        "                                               torch.tensor(train_encodings['attention_mask']),\n",
        "                                               torch.tensor(train_labels))\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
        "                                              torch.tensor(test_encodings['attention_mask']),\n",
        "                                              torch.tensor(test_labels))\n",
        "\n",
        "# Load the pre-trained BERT model and modify the last layer for classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_map))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T18:23:46.225342Z",
          "iopub.execute_input": "2023-03-10T18:23:46.225972Z",
          "iopub.status.idle": "2023-03-10T18:23:52.29324Z",
          "shell.execute_reply.started": "2023-03-10T18:23:46.225933Z",
          "shell.execute_reply": "2023-03-10T18:23:52.292165Z"
        },
        "trusted": true,
        "id": "IeZCrwHlq194",
        "outputId": "a53b3d0e-a3dc-4836-8d65-be5d28ea4cb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Get the input_ids, attention_mask, and labels from your dataset\n",
        "# Assuming you want to move a batch of data to the device\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
        "\n",
        "# Get a batch of data\n",
        "input_ids, attention_mask, labels = next(iter(train_dataloader))\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "labels = labels.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T18:26:09.680819Z",
          "iopub.execute_input": "2023-03-10T18:26:09.68153Z",
          "iopub.status.idle": "2023-03-10T18:26:09.810274Z",
          "shell.execute_reply.started": "2023-03-10T18:26:09.681493Z",
          "shell.execute_reply": "2023-03-10T18:26:09.809074Z"
        },
        "trusted": true,
        "id": "FYbpZOOsq194"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming `train_dataset`, `model`, and necessary imports are already defined\n",
        "\n",
        "# DataLoader for training dataset\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Set device for model training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Optimizer\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Number of epochs\n",
        "epochs = 5\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    epoch_loss = 0  # Initialize epoch loss\n",
        "    num_batches = len(train_loader)  # Number of batches in the epoch\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optim.zero_grad()  # Zero the gradients\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        # Accumulate loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Print progress every 10 batches (adjust as needed)\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{epochs}], Batch [{batch_idx + 1}/{num_batches}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    avg_epoch_loss = epoch_loss / num_batches\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}] completed. Average Loss: {avg_epoch_loss:.4f}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T18:26:11.763381Z",
          "iopub.execute_input": "2023-03-10T18:26:11.764118Z",
          "iopub.status.idle": "2023-03-10T19:20:58.058264Z",
          "shell.execute_reply.started": "2023-03-10T18:26:11.764074Z",
          "shell.execute_reply": "2023-03-10T19:20:58.056967Z"
        },
        "trusted": true,
        "id": "KjREfCcUq195",
        "outputId": "46bb1124-5f12-4d5b-877b-7732b0c1045f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Batch [10/1271], Loss: 1.7954\n",
            "Epoch [1/5], Batch [20/1271], Loss: 1.7410\n",
            "Epoch [1/5], Batch [30/1271], Loss: 1.5034\n",
            "Epoch [1/5], Batch [40/1271], Loss: 1.3013\n",
            "Epoch [1/5], Batch [50/1271], Loss: 1.1070\n",
            "Epoch [1/5], Batch [60/1271], Loss: 1.4066\n",
            "Epoch [1/5], Batch [70/1271], Loss: 1.1580\n",
            "Epoch [1/5], Batch [80/1271], Loss: 0.7662\n",
            "Epoch [1/5], Batch [90/1271], Loss: 0.7668\n",
            "Epoch [1/5], Batch [100/1271], Loss: 0.6214\n",
            "Epoch [1/5], Batch [110/1271], Loss: 0.8271\n",
            "Epoch [1/5], Batch [120/1271], Loss: 0.7836\n",
            "Epoch [1/5], Batch [130/1271], Loss: 0.6712\n",
            "Epoch [1/5], Batch [140/1271], Loss: 0.4897\n",
            "Epoch [1/5], Batch [150/1271], Loss: 0.6824\n",
            "Epoch [1/5], Batch [160/1271], Loss: 0.6519\n",
            "Epoch [1/5], Batch [170/1271], Loss: 0.5734\n",
            "Epoch [1/5], Batch [180/1271], Loss: 0.4818\n",
            "Epoch [1/5], Batch [190/1271], Loss: 0.6122\n",
            "Epoch [1/5], Batch [200/1271], Loss: 0.5601\n",
            "Epoch [1/5], Batch [210/1271], Loss: 1.0021\n",
            "Epoch [1/5], Batch [220/1271], Loss: 0.5693\n",
            "Epoch [1/5], Batch [230/1271], Loss: 0.9931\n",
            "Epoch [1/5], Batch [240/1271], Loss: 0.8069\n",
            "Epoch [1/5], Batch [250/1271], Loss: 0.5665\n",
            "Epoch [1/5], Batch [260/1271], Loss: 0.4145\n",
            "Epoch [1/5], Batch [270/1271], Loss: 0.8449\n",
            "Epoch [1/5], Batch [280/1271], Loss: 0.3553\n",
            "Epoch [1/5], Batch [290/1271], Loss: 0.5477\n",
            "Epoch [1/5], Batch [300/1271], Loss: 0.6865\n",
            "Epoch [1/5], Batch [310/1271], Loss: 0.3846\n",
            "Epoch [1/5], Batch [320/1271], Loss: 0.6569\n",
            "Epoch [1/5], Batch [330/1271], Loss: 0.2905\n",
            "Epoch [1/5], Batch [340/1271], Loss: 0.1822\n",
            "Epoch [1/5], Batch [350/1271], Loss: 0.7202\n",
            "Epoch [1/5], Batch [360/1271], Loss: 0.5412\n",
            "Epoch [1/5], Batch [370/1271], Loss: 1.0709\n",
            "Epoch [1/5], Batch [380/1271], Loss: 0.3555\n",
            "Epoch [1/5], Batch [390/1271], Loss: 0.3579\n",
            "Epoch [1/5], Batch [400/1271], Loss: 0.9902\n",
            "Epoch [1/5], Batch [410/1271], Loss: 0.7793\n",
            "Epoch [1/5], Batch [420/1271], Loss: 0.8450\n",
            "Epoch [1/5], Batch [430/1271], Loss: 0.9059\n",
            "Epoch [1/5], Batch [440/1271], Loss: 0.6389\n",
            "Epoch [1/5], Batch [450/1271], Loss: 0.6416\n",
            "Epoch [1/5], Batch [460/1271], Loss: 0.4759\n",
            "Epoch [1/5], Batch [470/1271], Loss: 0.1835\n",
            "Epoch [1/5], Batch [480/1271], Loss: 0.8953\n",
            "Epoch [1/5], Batch [490/1271], Loss: 0.9730\n",
            "Epoch [1/5], Batch [500/1271], Loss: 0.0797\n",
            "Epoch [1/5], Batch [510/1271], Loss: 0.6996\n",
            "Epoch [1/5], Batch [520/1271], Loss: 0.6084\n",
            "Epoch [1/5], Batch [530/1271], Loss: 0.6006\n",
            "Epoch [1/5], Batch [540/1271], Loss: 0.6241\n",
            "Epoch [1/5], Batch [550/1271], Loss: 0.5842\n",
            "Epoch [1/5], Batch [560/1271], Loss: 0.5182\n",
            "Epoch [1/5], Batch [570/1271], Loss: 0.8097\n",
            "Epoch [1/5], Batch [580/1271], Loss: 0.7714\n",
            "Epoch [1/5], Batch [590/1271], Loss: 0.5783\n",
            "Epoch [1/5], Batch [600/1271], Loss: 0.5592\n",
            "Epoch [1/5], Batch [610/1271], Loss: 0.9933\n",
            "Epoch [1/5], Batch [620/1271], Loss: 0.5832\n",
            "Epoch [1/5], Batch [630/1271], Loss: 0.6344\n",
            "Epoch [1/5], Batch [640/1271], Loss: 0.9226\n",
            "Epoch [1/5], Batch [650/1271], Loss: 0.5341\n",
            "Epoch [1/5], Batch [660/1271], Loss: 0.3871\n",
            "Epoch [1/5], Batch [670/1271], Loss: 0.5080\n",
            "Epoch [1/5], Batch [680/1271], Loss: 0.5454\n",
            "Epoch [1/5], Batch [690/1271], Loss: 0.7917\n",
            "Epoch [1/5], Batch [700/1271], Loss: 0.5266\n",
            "Epoch [1/5], Batch [710/1271], Loss: 0.3440\n",
            "Epoch [1/5], Batch [720/1271], Loss: 0.1247\n",
            "Epoch [1/5], Batch [730/1271], Loss: 0.5405\n",
            "Epoch [1/5], Batch [740/1271], Loss: 0.5269\n",
            "Epoch [1/5], Batch [750/1271], Loss: 0.5231\n",
            "Epoch [1/5], Batch [760/1271], Loss: 0.2300\n",
            "Epoch [1/5], Batch [770/1271], Loss: 1.0625\n",
            "Epoch [1/5], Batch [780/1271], Loss: 0.5129\n",
            "Epoch [1/5], Batch [790/1271], Loss: 0.9071\n",
            "Epoch [1/5], Batch [800/1271], Loss: 0.5156\n",
            "Epoch [1/5], Batch [810/1271], Loss: 0.5041\n",
            "Epoch [1/5], Batch [820/1271], Loss: 0.5888\n",
            "Epoch [1/5], Batch [830/1271], Loss: 0.2506\n",
            "Epoch [1/5], Batch [840/1271], Loss: 1.5739\n",
            "Epoch [1/5], Batch [850/1271], Loss: 0.4032\n",
            "Epoch [1/5], Batch [860/1271], Loss: 0.8303\n",
            "Epoch [1/5], Batch [870/1271], Loss: 0.4712\n",
            "Epoch [1/5], Batch [880/1271], Loss: 0.2503\n",
            "Epoch [1/5], Batch [890/1271], Loss: 0.2749\n",
            "Epoch [1/5], Batch [900/1271], Loss: 0.3201\n",
            "Epoch [1/5], Batch [910/1271], Loss: 0.2710\n",
            "Epoch [1/5], Batch [920/1271], Loss: 0.9608\n",
            "Epoch [1/5], Batch [930/1271], Loss: 0.8293\n",
            "Epoch [1/5], Batch [940/1271], Loss: 0.2764\n",
            "Epoch [1/5], Batch [950/1271], Loss: 0.3788\n",
            "Epoch [1/5], Batch [960/1271], Loss: 0.4955\n",
            "Epoch [1/5], Batch [970/1271], Loss: 0.6914\n",
            "Epoch [1/5], Batch [980/1271], Loss: 0.4921\n",
            "Epoch [1/5], Batch [990/1271], Loss: 0.3542\n",
            "Epoch [1/5], Batch [1000/1271], Loss: 0.7377\n",
            "Epoch [1/5], Batch [1010/1271], Loss: 0.4305\n",
            "Epoch [1/5], Batch [1020/1271], Loss: 1.0074\n",
            "Epoch [1/5], Batch [1030/1271], Loss: 1.0775\n",
            "Epoch [1/5], Batch [1040/1271], Loss: 0.2857\n",
            "Epoch [1/5], Batch [1050/1271], Loss: 0.4167\n",
            "Epoch [1/5], Batch [1060/1271], Loss: 0.5003\n",
            "Epoch [1/5], Batch [1070/1271], Loss: 0.2142\n",
            "Epoch [1/5], Batch [1080/1271], Loss: 0.3330\n",
            "Epoch [1/5], Batch [1090/1271], Loss: 0.6950\n",
            "Epoch [1/5], Batch [1100/1271], Loss: 0.5928\n",
            "Epoch [1/5], Batch [1110/1271], Loss: 0.4286\n",
            "Epoch [1/5], Batch [1120/1271], Loss: 0.6731\n",
            "Epoch [1/5], Batch [1130/1271], Loss: 0.8049\n",
            "Epoch [1/5], Batch [1140/1271], Loss: 1.0757\n",
            "Epoch [1/5], Batch [1150/1271], Loss: 0.4987\n",
            "Epoch [1/5], Batch [1160/1271], Loss: 0.3624\n",
            "Epoch [1/5], Batch [1170/1271], Loss: 1.1688\n",
            "Epoch [1/5], Batch [1180/1271], Loss: 0.3713\n",
            "Epoch [1/5], Batch [1190/1271], Loss: 0.3798\n",
            "Epoch [1/5], Batch [1200/1271], Loss: 0.5675\n",
            "Epoch [1/5], Batch [1210/1271], Loss: 0.6910\n",
            "Epoch [1/5], Batch [1220/1271], Loss: 0.7357\n",
            "Epoch [1/5], Batch [1230/1271], Loss: 0.3771\n",
            "Epoch [1/5], Batch [1240/1271], Loss: 0.5137\n",
            "Epoch [1/5], Batch [1250/1271], Loss: 0.5083\n",
            "Epoch [1/5], Batch [1260/1271], Loss: 0.6659\n",
            "Epoch [1/5], Batch [1270/1271], Loss: 0.8186\n",
            "Epoch [1/5] completed. Average Loss: 0.6696\n",
            "Epoch [2/5], Batch [10/1271], Loss: 0.3830\n",
            "Epoch [2/5], Batch [20/1271], Loss: 0.3029\n",
            "Epoch [2/5], Batch [30/1271], Loss: 0.5042\n",
            "Epoch [2/5], Batch [40/1271], Loss: 0.7614\n",
            "Epoch [2/5], Batch [50/1271], Loss: 0.0408\n",
            "Epoch [2/5], Batch [60/1271], Loss: 0.6069\n",
            "Epoch [2/5], Batch [70/1271], Loss: 0.1181\n",
            "Epoch [2/5], Batch [80/1271], Loss: 0.5673\n",
            "Epoch [2/5], Batch [90/1271], Loss: 0.3071\n",
            "Epoch [2/5], Batch [100/1271], Loss: 0.7996\n",
            "Epoch [2/5], Batch [110/1271], Loss: 0.4735\n",
            "Epoch [2/5], Batch [120/1271], Loss: 0.3079\n",
            "Epoch [2/5], Batch [130/1271], Loss: 0.3536\n",
            "Epoch [2/5], Batch [140/1271], Loss: 1.0518\n",
            "Epoch [2/5], Batch [150/1271], Loss: 0.2478\n",
            "Epoch [2/5], Batch [160/1271], Loss: 0.4459\n",
            "Epoch [2/5], Batch [170/1271], Loss: 0.6403\n",
            "Epoch [2/5], Batch [180/1271], Loss: 0.2177\n",
            "Epoch [2/5], Batch [190/1271], Loss: 0.2724\n",
            "Epoch [2/5], Batch [200/1271], Loss: 0.3576\n",
            "Epoch [2/5], Batch [210/1271], Loss: 0.4320\n",
            "Epoch [2/5], Batch [220/1271], Loss: 0.2253\n",
            "Epoch [2/5], Batch [230/1271], Loss: 0.2739\n",
            "Epoch [2/5], Batch [240/1271], Loss: 0.5513\n",
            "Epoch [2/5], Batch [250/1271], Loss: 0.1871\n",
            "Epoch [2/5], Batch [260/1271], Loss: 0.5350\n",
            "Epoch [2/5], Batch [270/1271], Loss: 0.4963\n",
            "Epoch [2/5], Batch [280/1271], Loss: 0.1299\n",
            "Epoch [2/5], Batch [290/1271], Loss: 0.6082\n",
            "Epoch [2/5], Batch [300/1271], Loss: 0.2180\n",
            "Epoch [2/5], Batch [310/1271], Loss: 0.4660\n",
            "Epoch [2/5], Batch [320/1271], Loss: 0.4053\n",
            "Epoch [2/5], Batch [330/1271], Loss: 0.1880\n",
            "Epoch [2/5], Batch [340/1271], Loss: 0.1275\n",
            "Epoch [2/5], Batch [350/1271], Loss: 0.4024\n",
            "Epoch [2/5], Batch [360/1271], Loss: 0.5478\n",
            "Epoch [2/5], Batch [370/1271], Loss: 0.4892\n",
            "Epoch [2/5], Batch [380/1271], Loss: 0.2479\n",
            "Epoch [2/5], Batch [390/1271], Loss: 0.4937\n",
            "Epoch [2/5], Batch [400/1271], Loss: 0.1575\n",
            "Epoch [2/5], Batch [410/1271], Loss: 0.4763\n",
            "Epoch [2/5], Batch [420/1271], Loss: 0.4636\n",
            "Epoch [2/5], Batch [430/1271], Loss: 0.2162\n",
            "Epoch [2/5], Batch [440/1271], Loss: 0.1080\n",
            "Epoch [2/5], Batch [450/1271], Loss: 0.0503\n",
            "Epoch [2/5], Batch [460/1271], Loss: 0.5758\n",
            "Epoch [2/5], Batch [470/1271], Loss: 0.2269\n",
            "Epoch [2/5], Batch [480/1271], Loss: 0.1670\n",
            "Epoch [2/5], Batch [490/1271], Loss: 0.2571\n",
            "Epoch [2/5], Batch [500/1271], Loss: 0.5234\n",
            "Epoch [2/5], Batch [510/1271], Loss: 0.2597\n",
            "Epoch [2/5], Batch [520/1271], Loss: 0.3766\n",
            "Epoch [2/5], Batch [530/1271], Loss: 0.3097\n",
            "Epoch [2/5], Batch [540/1271], Loss: 0.5368\n",
            "Epoch [2/5], Batch [550/1271], Loss: 0.1303\n",
            "Epoch [2/5], Batch [560/1271], Loss: 0.2650\n",
            "Epoch [2/5], Batch [570/1271], Loss: 0.3625\n",
            "Epoch [2/5], Batch [580/1271], Loss: 0.5307\n",
            "Epoch [2/5], Batch [590/1271], Loss: 0.1996\n",
            "Epoch [2/5], Batch [600/1271], Loss: 0.8419\n",
            "Epoch [2/5], Batch [610/1271], Loss: 0.4954\n",
            "Epoch [2/5], Batch [620/1271], Loss: 0.4763\n",
            "Epoch [2/5], Batch [630/1271], Loss: 0.1407\n",
            "Epoch [2/5], Batch [640/1271], Loss: 0.7344\n",
            "Epoch [2/5], Batch [650/1271], Loss: 0.2338\n",
            "Epoch [2/5], Batch [660/1271], Loss: 0.3390\n",
            "Epoch [2/5], Batch [670/1271], Loss: 0.1582\n",
            "Epoch [2/5], Batch [680/1271], Loss: 0.1019\n",
            "Epoch [2/5], Batch [690/1271], Loss: 0.3027\n",
            "Epoch [2/5], Batch [700/1271], Loss: 0.3096\n",
            "Epoch [2/5], Batch [710/1271], Loss: 0.4677\n",
            "Epoch [2/5], Batch [720/1271], Loss: 0.3864\n",
            "Epoch [2/5], Batch [730/1271], Loss: 0.1670\n",
            "Epoch [2/5], Batch [740/1271], Loss: 0.2292\n",
            "Epoch [2/5], Batch [750/1271], Loss: 0.1002\n",
            "Epoch [2/5], Batch [760/1271], Loss: 0.2961\n",
            "Epoch [2/5], Batch [770/1271], Loss: 0.6171\n",
            "Epoch [2/5], Batch [780/1271], Loss: 0.5903\n",
            "Epoch [2/5], Batch [790/1271], Loss: 0.7046\n",
            "Epoch [2/5], Batch [800/1271], Loss: 0.4756\n",
            "Epoch [2/5], Batch [810/1271], Loss: 0.3795\n",
            "Epoch [2/5], Batch [820/1271], Loss: 0.2595\n",
            "Epoch [2/5], Batch [830/1271], Loss: 0.5753\n",
            "Epoch [2/5], Batch [840/1271], Loss: 0.8592\n",
            "Epoch [2/5], Batch [850/1271], Loss: 0.4786\n",
            "Epoch [2/5], Batch [860/1271], Loss: 0.4661\n",
            "Epoch [2/5], Batch [870/1271], Loss: 0.1383\n",
            "Epoch [2/5], Batch [880/1271], Loss: 0.6146\n",
            "Epoch [2/5], Batch [890/1271], Loss: 0.3896\n",
            "Epoch [2/5], Batch [900/1271], Loss: 0.4019\n",
            "Epoch [2/5], Batch [910/1271], Loss: 0.1888\n",
            "Epoch [2/5], Batch [920/1271], Loss: 0.4699\n",
            "Epoch [2/5], Batch [930/1271], Loss: 0.1953\n",
            "Epoch [2/5], Batch [940/1271], Loss: 0.2212\n",
            "Epoch [2/5], Batch [950/1271], Loss: 0.2021\n",
            "Epoch [2/5], Batch [960/1271], Loss: 0.2599\n",
            "Epoch [2/5], Batch [970/1271], Loss: 0.2424\n",
            "Epoch [2/5], Batch [980/1271], Loss: 0.5241\n",
            "Epoch [2/5], Batch [990/1271], Loss: 0.5155\n",
            "Epoch [2/5], Batch [1000/1271], Loss: 0.4950\n",
            "Epoch [2/5], Batch [1010/1271], Loss: 0.4783\n",
            "Epoch [2/5], Batch [1020/1271], Loss: 0.1823\n",
            "Epoch [2/5], Batch [1030/1271], Loss: 0.1990\n",
            "Epoch [2/5], Batch [1040/1271], Loss: 0.6166\n",
            "Epoch [2/5], Batch [1050/1271], Loss: 0.0974\n",
            "Epoch [2/5], Batch [1060/1271], Loss: 0.1968\n",
            "Epoch [2/5], Batch [1070/1271], Loss: 0.6270\n",
            "Epoch [2/5], Batch [1080/1271], Loss: 0.4393\n",
            "Epoch [2/5], Batch [1090/1271], Loss: 0.8198\n",
            "Epoch [2/5], Batch [1100/1271], Loss: 0.6354\n",
            "Epoch [2/5], Batch [1110/1271], Loss: 0.5067\n",
            "Epoch [2/5], Batch [1120/1271], Loss: 0.3999\n",
            "Epoch [2/5], Batch [1130/1271], Loss: 0.7492\n",
            "Epoch [2/5], Batch [1140/1271], Loss: 0.3841\n",
            "Epoch [2/5], Batch [1150/1271], Loss: 0.3948\n",
            "Epoch [2/5], Batch [1160/1271], Loss: 0.1510\n",
            "Epoch [2/5], Batch [1170/1271], Loss: 0.4507\n",
            "Epoch [2/5], Batch [1180/1271], Loss: 0.4582\n",
            "Epoch [2/5], Batch [1190/1271], Loss: 0.0728\n",
            "Epoch [2/5], Batch [1200/1271], Loss: 0.0859\n",
            "Epoch [2/5], Batch [1210/1271], Loss: 0.1967\n",
            "Epoch [2/5], Batch [1220/1271], Loss: 0.2416\n",
            "Epoch [2/5], Batch [1230/1271], Loss: 0.5324\n",
            "Epoch [2/5], Batch [1240/1271], Loss: 0.1173\n",
            "Epoch [2/5], Batch [1250/1271], Loss: 0.3919\n",
            "Epoch [2/5], Batch [1260/1271], Loss: 0.2336\n",
            "Epoch [2/5], Batch [1270/1271], Loss: 0.1567\n",
            "Epoch [2/5] completed. Average Loss: 0.3813\n",
            "Epoch [3/5], Batch [10/1271], Loss: 0.6261\n",
            "Epoch [3/5], Batch [20/1271], Loss: 0.1924\n",
            "Epoch [3/5], Batch [30/1271], Loss: 0.4116\n",
            "Epoch [3/5], Batch [40/1271], Loss: 0.0948\n",
            "Epoch [3/5], Batch [50/1271], Loss: 0.3615\n",
            "Epoch [3/5], Batch [60/1271], Loss: 0.1896\n",
            "Epoch [3/5], Batch [70/1271], Loss: 0.0729\n",
            "Epoch [3/5], Batch [80/1271], Loss: 0.0428\n",
            "Epoch [3/5], Batch [90/1271], Loss: 0.4752\n",
            "Epoch [3/5], Batch [100/1271], Loss: 0.1834\n",
            "Epoch [3/5], Batch [110/1271], Loss: 0.3663\n",
            "Epoch [3/5], Batch [120/1271], Loss: 0.5086\n",
            "Epoch [3/5], Batch [130/1271], Loss: 0.0370\n",
            "Epoch [3/5], Batch [140/1271], Loss: 0.0454\n",
            "Epoch [3/5], Batch [150/1271], Loss: 0.4918\n",
            "Epoch [3/5], Batch [160/1271], Loss: 0.2673\n",
            "Epoch [3/5], Batch [170/1271], Loss: 0.4594\n",
            "Epoch [3/5], Batch [180/1271], Loss: 0.2728\n",
            "Epoch [3/5], Batch [190/1271], Loss: 0.2183\n",
            "Epoch [3/5], Batch [200/1271], Loss: 0.0431\n",
            "Epoch [3/5], Batch [210/1271], Loss: 0.0434\n",
            "Epoch [3/5], Batch [220/1271], Loss: 0.1717\n",
            "Epoch [3/5], Batch [230/1271], Loss: 0.1773\n",
            "Epoch [3/5], Batch [240/1271], Loss: 0.1191\n",
            "Epoch [3/5], Batch [250/1271], Loss: 0.0374\n",
            "Epoch [3/5], Batch [260/1271], Loss: 0.2990\n",
            "Epoch [3/5], Batch [270/1271], Loss: 0.0899\n",
            "Epoch [3/5], Batch [280/1271], Loss: 0.1569\n",
            "Epoch [3/5], Batch [290/1271], Loss: 0.2344\n",
            "Epoch [3/5], Batch [300/1271], Loss: 0.0467\n",
            "Epoch [3/5], Batch [310/1271], Loss: 0.4962\n",
            "Epoch [3/5], Batch [320/1271], Loss: 0.1463\n",
            "Epoch [3/5], Batch [330/1271], Loss: 0.2947\n",
            "Epoch [3/5], Batch [340/1271], Loss: 0.5778\n",
            "Epoch [3/5], Batch [350/1271], Loss: 0.5250\n",
            "Epoch [3/5], Batch [360/1271], Loss: 0.2133\n",
            "Epoch [3/5], Batch [370/1271], Loss: 0.0362\n",
            "Epoch [3/5], Batch [380/1271], Loss: 0.3660\n",
            "Epoch [3/5], Batch [390/1271], Loss: 0.2111\n",
            "Epoch [3/5], Batch [400/1271], Loss: 0.2208\n",
            "Epoch [3/5], Batch [410/1271], Loss: 0.3129\n",
            "Epoch [3/5], Batch [420/1271], Loss: 0.2307\n",
            "Epoch [3/5], Batch [430/1271], Loss: 0.0814\n",
            "Epoch [3/5], Batch [440/1271], Loss: 0.1647\n",
            "Epoch [3/5], Batch [450/1271], Loss: 0.0403\n",
            "Epoch [3/5], Batch [460/1271], Loss: 0.0423\n",
            "Epoch [3/5], Batch [470/1271], Loss: 0.2502\n",
            "Epoch [3/5], Batch [480/1271], Loss: 0.1032\n",
            "Epoch [3/5], Batch [490/1271], Loss: 0.2852\n",
            "Epoch [3/5], Batch [500/1271], Loss: 0.2024\n",
            "Epoch [3/5], Batch [510/1271], Loss: 0.1267\n",
            "Epoch [3/5], Batch [520/1271], Loss: 0.4093\n",
            "Epoch [3/5], Batch [530/1271], Loss: 0.0502\n",
            "Epoch [3/5], Batch [540/1271], Loss: 0.4811\n",
            "Epoch [3/5], Batch [550/1271], Loss: 0.6475\n",
            "Epoch [3/5], Batch [560/1271], Loss: 0.0186\n",
            "Epoch [3/5], Batch [570/1271], Loss: 0.1513\n",
            "Epoch [3/5], Batch [580/1271], Loss: 0.1562\n",
            "Epoch [3/5], Batch [590/1271], Loss: 0.1289\n",
            "Epoch [3/5], Batch [600/1271], Loss: 0.1403\n",
            "Epoch [3/5], Batch [610/1271], Loss: 0.4149\n",
            "Epoch [3/5], Batch [620/1271], Loss: 0.0165\n",
            "Epoch [3/5], Batch [630/1271], Loss: 0.4962\n",
            "Epoch [3/5], Batch [640/1271], Loss: 0.3053\n",
            "Epoch [3/5], Batch [650/1271], Loss: 0.4498\n",
            "Epoch [3/5], Batch [660/1271], Loss: 0.8024\n",
            "Epoch [3/5], Batch [670/1271], Loss: 0.4166\n",
            "Epoch [3/5], Batch [680/1271], Loss: 0.1215\n",
            "Epoch [3/5], Batch [690/1271], Loss: 0.1073\n",
            "Epoch [3/5], Batch [700/1271], Loss: 0.2724\n",
            "Epoch [3/5], Batch [710/1271], Loss: 0.2344\n",
            "Epoch [3/5], Batch [720/1271], Loss: 0.0351\n",
            "Epoch [3/5], Batch [730/1271], Loss: 0.3202\n",
            "Epoch [3/5], Batch [740/1271], Loss: 0.0891\n",
            "Epoch [3/5], Batch [750/1271], Loss: 0.3019\n",
            "Epoch [3/5], Batch [760/1271], Loss: 0.5295\n",
            "Epoch [3/5], Batch [770/1271], Loss: 0.1864\n",
            "Epoch [3/5], Batch [780/1271], Loss: 0.0929\n",
            "Epoch [3/5], Batch [790/1271], Loss: 0.0606\n",
            "Epoch [3/5], Batch [800/1271], Loss: 0.1850\n",
            "Epoch [3/5], Batch [810/1271], Loss: 0.5814\n",
            "Epoch [3/5], Batch [820/1271], Loss: 0.0442\n",
            "Epoch [3/5], Batch [830/1271], Loss: 0.0637\n",
            "Epoch [3/5], Batch [840/1271], Loss: 0.0655\n",
            "Epoch [3/5], Batch [850/1271], Loss: 0.2655\n",
            "Epoch [3/5], Batch [860/1271], Loss: 0.2994\n",
            "Epoch [3/5], Batch [870/1271], Loss: 0.2326\n",
            "Epoch [3/5], Batch [880/1271], Loss: 0.2751\n",
            "Epoch [3/5], Batch [890/1271], Loss: 0.2133\n",
            "Epoch [3/5], Batch [900/1271], Loss: 0.0771\n",
            "Epoch [3/5], Batch [910/1271], Loss: 0.1395\n",
            "Epoch [3/5], Batch [920/1271], Loss: 0.3631\n",
            "Epoch [3/5], Batch [930/1271], Loss: 0.0718\n",
            "Epoch [3/5], Batch [940/1271], Loss: 0.0827\n",
            "Epoch [3/5], Batch [950/1271], Loss: 0.0591\n",
            "Epoch [3/5], Batch [960/1271], Loss: 0.0699\n",
            "Epoch [3/5], Batch [970/1271], Loss: 0.1664\n",
            "Epoch [3/5], Batch [980/1271], Loss: 0.1676\n",
            "Epoch [3/5], Batch [990/1271], Loss: 0.0912\n",
            "Epoch [3/5], Batch [1000/1271], Loss: 0.0325\n",
            "Epoch [3/5], Batch [1010/1271], Loss: 0.2475\n",
            "Epoch [3/5], Batch [1020/1271], Loss: 0.2314\n",
            "Epoch [3/5], Batch [1030/1271], Loss: 0.0682\n",
            "Epoch [3/5], Batch [1040/1271], Loss: 0.1982\n",
            "Epoch [3/5], Batch [1050/1271], Loss: 0.6120\n",
            "Epoch [3/5], Batch [1060/1271], Loss: 0.1074\n",
            "Epoch [3/5], Batch [1070/1271], Loss: 0.3148\n",
            "Epoch [3/5], Batch [1080/1271], Loss: 0.1137\n",
            "Epoch [3/5], Batch [1090/1271], Loss: 0.3887\n",
            "Epoch [3/5], Batch [1100/1271], Loss: 0.1655\n",
            "Epoch [3/5], Batch [1110/1271], Loss: 0.1434\n",
            "Epoch [3/5], Batch [1120/1271], Loss: 0.6392\n",
            "Epoch [3/5], Batch [1130/1271], Loss: 0.0279\n",
            "Epoch [3/5], Batch [1140/1271], Loss: 0.0933\n",
            "Epoch [3/5], Batch [1150/1271], Loss: 0.1518\n",
            "Epoch [3/5], Batch [1160/1271], Loss: 0.5032\n",
            "Epoch [3/5], Batch [1170/1271], Loss: 0.5490\n",
            "Epoch [3/5], Batch [1180/1271], Loss: 0.2306\n",
            "Epoch [3/5], Batch [1190/1271], Loss: 0.2116\n",
            "Epoch [3/5], Batch [1200/1271], Loss: 1.0651\n",
            "Epoch [3/5], Batch [1210/1271], Loss: 0.1295\n",
            "Epoch [3/5], Batch [1220/1271], Loss: 0.1469\n",
            "Epoch [3/5], Batch [1230/1271], Loss: 0.1242\n",
            "Epoch [3/5], Batch [1240/1271], Loss: 0.1770\n",
            "Epoch [3/5], Batch [1250/1271], Loss: 0.7522\n",
            "Epoch [3/5], Batch [1260/1271], Loss: 0.1584\n",
            "Epoch [3/5], Batch [1270/1271], Loss: 0.2638\n",
            "Epoch [3/5] completed. Average Loss: 0.2420\n",
            "Epoch [4/5], Batch [10/1271], Loss: 0.1149\n",
            "Epoch [4/5], Batch [20/1271], Loss: 0.0518\n",
            "Epoch [4/5], Batch [30/1271], Loss: 0.0392\n",
            "Epoch [4/5], Batch [40/1271], Loss: 0.0630\n",
            "Epoch [4/5], Batch [50/1271], Loss: 0.1168\n",
            "Epoch [4/5], Batch [60/1271], Loss: 0.0895\n",
            "Epoch [4/5], Batch [70/1271], Loss: 0.1297\n",
            "Epoch [4/5], Batch [80/1271], Loss: 0.0177\n",
            "Epoch [4/5], Batch [90/1271], Loss: 0.1257\n",
            "Epoch [4/5], Batch [100/1271], Loss: 0.3302\n",
            "Epoch [4/5], Batch [110/1271], Loss: 0.0723\n",
            "Epoch [4/5], Batch [120/1271], Loss: 0.1909\n",
            "Epoch [4/5], Batch [130/1271], Loss: 0.2831\n",
            "Epoch [4/5], Batch [140/1271], Loss: 0.2063\n",
            "Epoch [4/5], Batch [150/1271], Loss: 0.0372\n",
            "Epoch [4/5], Batch [160/1271], Loss: 0.0688\n",
            "Epoch [4/5], Batch [170/1271], Loss: 0.0144\n",
            "Epoch [4/5], Batch [180/1271], Loss: 0.2893\n",
            "Epoch [4/5], Batch [190/1271], Loss: 0.0124\n",
            "Epoch [4/5], Batch [200/1271], Loss: 0.0127\n",
            "Epoch [4/5], Batch [210/1271], Loss: 0.0585\n",
            "Epoch [4/5], Batch [220/1271], Loss: 0.3951\n",
            "Epoch [4/5], Batch [230/1271], Loss: 0.3903\n",
            "Epoch [4/5], Batch [240/1271], Loss: 0.2136\n",
            "Epoch [4/5], Batch [250/1271], Loss: 0.0418\n",
            "Epoch [4/5], Batch [260/1271], Loss: 0.1835\n",
            "Epoch [4/5], Batch [270/1271], Loss: 0.0284\n",
            "Epoch [4/5], Batch [280/1271], Loss: 0.0266\n",
            "Epoch [4/5], Batch [290/1271], Loss: 0.0088\n",
            "Epoch [4/5], Batch [300/1271], Loss: 0.4474\n",
            "Epoch [4/5], Batch [310/1271], Loss: 0.0604\n",
            "Epoch [4/5], Batch [320/1271], Loss: 0.5554\n",
            "Epoch [4/5], Batch [330/1271], Loss: 0.2110\n",
            "Epoch [4/5], Batch [340/1271], Loss: 0.0458\n",
            "Epoch [4/5], Batch [350/1271], Loss: 0.0621\n",
            "Epoch [4/5], Batch [360/1271], Loss: 0.0491\n",
            "Epoch [4/5], Batch [370/1271], Loss: 0.0745\n",
            "Epoch [4/5], Batch [380/1271], Loss: 0.1661\n",
            "Epoch [4/5], Batch [390/1271], Loss: 0.0412\n",
            "Epoch [4/5], Batch [400/1271], Loss: 0.3436\n",
            "Epoch [4/5], Batch [410/1271], Loss: 0.0639\n",
            "Epoch [4/5], Batch [420/1271], Loss: 0.1725\n",
            "Epoch [4/5], Batch [430/1271], Loss: 0.2136\n",
            "Epoch [4/5], Batch [440/1271], Loss: 0.4850\n",
            "Epoch [4/5], Batch [450/1271], Loss: 0.2338\n",
            "Epoch [4/5], Batch [460/1271], Loss: 0.0309\n",
            "Epoch [4/5], Batch [470/1271], Loss: 0.0076\n",
            "Epoch [4/5], Batch [480/1271], Loss: 0.0892\n",
            "Epoch [4/5], Batch [490/1271], Loss: 0.0378\n",
            "Epoch [4/5], Batch [500/1271], Loss: 0.0345\n",
            "Epoch [4/5], Batch [510/1271], Loss: 0.1078\n",
            "Epoch [4/5], Batch [520/1271], Loss: 0.1009\n",
            "Epoch [4/5], Batch [530/1271], Loss: 0.0326\n",
            "Epoch [4/5], Batch [540/1271], Loss: 0.0956\n",
            "Epoch [4/5], Batch [550/1271], Loss: 0.2308\n",
            "Epoch [4/5], Batch [560/1271], Loss: 0.0048\n",
            "Epoch [4/5], Batch [570/1271], Loss: 0.1660\n",
            "Epoch [4/5], Batch [580/1271], Loss: 0.0151\n",
            "Epoch [4/5], Batch [590/1271], Loss: 0.1823\n",
            "Epoch [4/5], Batch [600/1271], Loss: 0.0172\n",
            "Epoch [4/5], Batch [610/1271], Loss: 0.0166\n",
            "Epoch [4/5], Batch [620/1271], Loss: 0.0300\n",
            "Epoch [4/5], Batch [630/1271], Loss: 0.3796\n",
            "Epoch [4/5], Batch [640/1271], Loss: 0.2293\n",
            "Epoch [4/5], Batch [650/1271], Loss: 0.2897\n",
            "Epoch [4/5], Batch [660/1271], Loss: 0.1930\n",
            "Epoch [4/5], Batch [670/1271], Loss: 0.0808\n",
            "Epoch [4/5], Batch [680/1271], Loss: 0.0485\n",
            "Epoch [4/5], Batch [690/1271], Loss: 0.2655\n",
            "Epoch [4/5], Batch [700/1271], Loss: 0.0232\n",
            "Epoch [4/5], Batch [710/1271], Loss: 0.0070\n",
            "Epoch [4/5], Batch [720/1271], Loss: 0.0139\n",
            "Epoch [4/5], Batch [730/1271], Loss: 0.2313\n",
            "Epoch [4/5], Batch [740/1271], Loss: 0.0692\n",
            "Epoch [4/5], Batch [750/1271], Loss: 0.0342\n",
            "Epoch [4/5], Batch [760/1271], Loss: 0.0196\n",
            "Epoch [4/5], Batch [770/1271], Loss: 0.0185\n",
            "Epoch [4/5], Batch [780/1271], Loss: 0.0475\n",
            "Epoch [4/5], Batch [790/1271], Loss: 0.0653\n",
            "Epoch [4/5], Batch [800/1271], Loss: 0.0105\n",
            "Epoch [4/5], Batch [810/1271], Loss: 0.1099\n",
            "Epoch [4/5], Batch [820/1271], Loss: 0.4314\n",
            "Epoch [4/5], Batch [830/1271], Loss: 0.3440\n",
            "Epoch [4/5], Batch [840/1271], Loss: 0.2995\n",
            "Epoch [4/5], Batch [850/1271], Loss: 0.1322\n",
            "Epoch [4/5], Batch [860/1271], Loss: 0.2003\n",
            "Epoch [4/5], Batch [870/1271], Loss: 0.0685\n",
            "Epoch [4/5], Batch [880/1271], Loss: 0.0120\n",
            "Epoch [4/5], Batch [890/1271], Loss: 0.0318\n",
            "Epoch [4/5], Batch [900/1271], Loss: 0.1027\n",
            "Epoch [4/5], Batch [910/1271], Loss: 0.0183\n",
            "Epoch [4/5], Batch [920/1271], Loss: 0.1238\n",
            "Epoch [4/5], Batch [930/1271], Loss: 0.1979\n",
            "Epoch [4/5], Batch [940/1271], Loss: 0.1184\n",
            "Epoch [4/5], Batch [950/1271], Loss: 0.1174\n",
            "Epoch [4/5], Batch [960/1271], Loss: 0.3405\n",
            "Epoch [4/5], Batch [970/1271], Loss: 0.6099\n",
            "Epoch [4/5], Batch [980/1271], Loss: 0.1449\n",
            "Epoch [4/5], Batch [990/1271], Loss: 0.2957\n",
            "Epoch [4/5], Batch [1000/1271], Loss: 0.6204\n",
            "Epoch [4/5], Batch [1010/1271], Loss: 0.1998\n",
            "Epoch [4/5], Batch [1020/1271], Loss: 0.3678\n",
            "Epoch [4/5], Batch [1030/1271], Loss: 0.5286\n",
            "Epoch [4/5], Batch [1040/1271], Loss: 0.1979\n",
            "Epoch [4/5], Batch [1050/1271], Loss: 0.0138\n",
            "Epoch [4/5], Batch [1060/1271], Loss: 0.3567\n",
            "Epoch [4/5], Batch [1070/1271], Loss: 0.0232\n",
            "Epoch [4/5], Batch [1080/1271], Loss: 0.1353\n",
            "Epoch [4/5], Batch [1090/1271], Loss: 0.2209\n",
            "Epoch [4/5], Batch [1100/1271], Loss: 0.0531\n",
            "Epoch [4/5], Batch [1110/1271], Loss: 0.0093\n",
            "Epoch [4/5], Batch [1120/1271], Loss: 0.0330\n",
            "Epoch [4/5], Batch [1130/1271], Loss: 0.1557\n",
            "Epoch [4/5], Batch [1140/1271], Loss: 0.0577\n",
            "Epoch [4/5], Batch [1150/1271], Loss: 0.0822\n",
            "Epoch [4/5], Batch [1160/1271], Loss: 0.3605\n",
            "Epoch [4/5], Batch [1170/1271], Loss: 0.1609\n",
            "Epoch [4/5], Batch [1180/1271], Loss: 0.0120\n",
            "Epoch [4/5], Batch [1190/1271], Loss: 0.1364\n",
            "Epoch [4/5], Batch [1200/1271], Loss: 0.5707\n",
            "Epoch [4/5], Batch [1210/1271], Loss: 0.6486\n",
            "Epoch [4/5], Batch [1220/1271], Loss: 0.0487\n",
            "Epoch [4/5], Batch [1230/1271], Loss: 0.2178\n",
            "Epoch [4/5], Batch [1240/1271], Loss: 0.3088\n",
            "Epoch [4/5], Batch [1250/1271], Loss: 0.0835\n",
            "Epoch [4/5], Batch [1260/1271], Loss: 0.0187\n",
            "Epoch [4/5], Batch [1270/1271], Loss: 0.1135\n",
            "Epoch [4/5] completed. Average Loss: 0.1590\n",
            "Epoch [5/5], Batch [10/1271], Loss: 0.0106\n",
            "Epoch [5/5], Batch [20/1271], Loss: 0.3888\n",
            "Epoch [5/5], Batch [30/1271], Loss: 0.2157\n",
            "Epoch [5/5], Batch [40/1271], Loss: 0.0188\n",
            "Epoch [5/5], Batch [50/1271], Loss: 0.0283\n",
            "Epoch [5/5], Batch [60/1271], Loss: 0.0096\n",
            "Epoch [5/5], Batch [70/1271], Loss: 0.0064\n",
            "Epoch [5/5], Batch [80/1271], Loss: 0.0773\n",
            "Epoch [5/5], Batch [90/1271], Loss: 0.0042\n",
            "Epoch [5/5], Batch [100/1271], Loss: 0.0678\n",
            "Epoch [5/5], Batch [110/1271], Loss: 0.0193\n",
            "Epoch [5/5], Batch [120/1271], Loss: 0.1768\n",
            "Epoch [5/5], Batch [130/1271], Loss: 0.0486\n",
            "Epoch [5/5], Batch [140/1271], Loss: 0.0934\n",
            "Epoch [5/5], Batch [150/1271], Loss: 0.0094\n",
            "Epoch [5/5], Batch [160/1271], Loss: 0.0122\n",
            "Epoch [5/5], Batch [170/1271], Loss: 0.0297\n",
            "Epoch [5/5], Batch [180/1271], Loss: 0.1176\n",
            "Epoch [5/5], Batch [190/1271], Loss: 0.0115\n",
            "Epoch [5/5], Batch [200/1271], Loss: 0.0405\n",
            "Epoch [5/5], Batch [210/1271], Loss: 0.0431\n",
            "Epoch [5/5], Batch [220/1271], Loss: 0.1025\n",
            "Epoch [5/5], Batch [230/1271], Loss: 0.0877\n",
            "Epoch [5/5], Batch [240/1271], Loss: 0.3739\n",
            "Epoch [5/5], Batch [250/1271], Loss: 0.1526\n",
            "Epoch [5/5], Batch [260/1271], Loss: 0.0219\n",
            "Epoch [5/5], Batch [270/1271], Loss: 0.2551\n",
            "Epoch [5/5], Batch [280/1271], Loss: 0.1010\n",
            "Epoch [5/5], Batch [290/1271], Loss: 0.0815\n",
            "Epoch [5/5], Batch [300/1271], Loss: 0.0873\n",
            "Epoch [5/5], Batch [310/1271], Loss: 0.0117\n",
            "Epoch [5/5], Batch [320/1271], Loss: 0.0169\n",
            "Epoch [5/5], Batch [330/1271], Loss: 0.0065\n",
            "Epoch [5/5], Batch [340/1271], Loss: 0.0606\n",
            "Epoch [5/5], Batch [350/1271], Loss: 0.0141\n",
            "Epoch [5/5], Batch [360/1271], Loss: 0.0491\n",
            "Epoch [5/5], Batch [370/1271], Loss: 0.0968\n",
            "Epoch [5/5], Batch [380/1271], Loss: 0.0997\n",
            "Epoch [5/5], Batch [390/1271], Loss: 0.1620\n",
            "Epoch [5/5], Batch [400/1271], Loss: 0.2598\n",
            "Epoch [5/5], Batch [410/1271], Loss: 0.0049\n",
            "Epoch [5/5], Batch [420/1271], Loss: 0.2140\n",
            "Epoch [5/5], Batch [430/1271], Loss: 0.0223\n",
            "Epoch [5/5], Batch [440/1271], Loss: 0.0483\n",
            "Epoch [5/5], Batch [450/1271], Loss: 0.0390\n",
            "Epoch [5/5], Batch [460/1271], Loss: 0.0257\n",
            "Epoch [5/5], Batch [470/1271], Loss: 0.4430\n",
            "Epoch [5/5], Batch [480/1271], Loss: 0.1405\n",
            "Epoch [5/5], Batch [490/1271], Loss: 0.0138\n",
            "Epoch [5/5], Batch [500/1271], Loss: 0.0136\n",
            "Epoch [5/5], Batch [510/1271], Loss: 0.1642\n",
            "Epoch [5/5], Batch [520/1271], Loss: 0.0801\n",
            "Epoch [5/5], Batch [530/1271], Loss: 0.0065\n",
            "Epoch [5/5], Batch [540/1271], Loss: 0.0154\n",
            "Epoch [5/5], Batch [550/1271], Loss: 0.0625\n",
            "Epoch [5/5], Batch [560/1271], Loss: 0.0640\n",
            "Epoch [5/5], Batch [570/1271], Loss: 0.0037\n",
            "Epoch [5/5], Batch [580/1271], Loss: 0.0130\n",
            "Epoch [5/5], Batch [590/1271], Loss: 0.0968\n",
            "Epoch [5/5], Batch [600/1271], Loss: 0.0570\n",
            "Epoch [5/5], Batch [610/1271], Loss: 0.1288\n",
            "Epoch [5/5], Batch [620/1271], Loss: 0.1253\n",
            "Epoch [5/5], Batch [630/1271], Loss: 0.0166\n",
            "Epoch [5/5], Batch [640/1271], Loss: 0.0167\n",
            "Epoch [5/5], Batch [650/1271], Loss: 0.0203\n",
            "Epoch [5/5], Batch [660/1271], Loss: 0.1369\n",
            "Epoch [5/5], Batch [670/1271], Loss: 0.0820\n",
            "Epoch [5/5], Batch [680/1271], Loss: 0.6821\n",
            "Epoch [5/5], Batch [690/1271], Loss: 0.0129\n",
            "Epoch [5/5], Batch [700/1271], Loss: 0.0511\n",
            "Epoch [5/5], Batch [710/1271], Loss: 0.0089\n",
            "Epoch [5/5], Batch [720/1271], Loss: 0.4418\n",
            "Epoch [5/5], Batch [730/1271], Loss: 0.0062\n",
            "Epoch [5/5], Batch [740/1271], Loss: 0.4088\n",
            "Epoch [5/5], Batch [750/1271], Loss: 0.0559\n",
            "Epoch [5/5], Batch [760/1271], Loss: 0.0383\n",
            "Epoch [5/5], Batch [770/1271], Loss: 0.0207\n",
            "Epoch [5/5], Batch [780/1271], Loss: 0.0555\n",
            "Epoch [5/5], Batch [790/1271], Loss: 0.1750\n",
            "Epoch [5/5], Batch [800/1271], Loss: 0.3363\n",
            "Epoch [5/5], Batch [810/1271], Loss: 0.2864\n",
            "Epoch [5/5], Batch [820/1271], Loss: 0.0482\n",
            "Epoch [5/5], Batch [830/1271], Loss: 0.2969\n",
            "Epoch [5/5], Batch [840/1271], Loss: 0.0386\n",
            "Epoch [5/5], Batch [850/1271], Loss: 0.0118\n",
            "Epoch [5/5], Batch [860/1271], Loss: 0.2513\n",
            "Epoch [5/5], Batch [870/1271], Loss: 0.0307\n",
            "Epoch [5/5], Batch [880/1271], Loss: 0.0672\n",
            "Epoch [5/5], Batch [890/1271], Loss: 0.0182\n",
            "Epoch [5/5], Batch [900/1271], Loss: 0.1852\n",
            "Epoch [5/5], Batch [910/1271], Loss: 0.0450\n",
            "Epoch [5/5], Batch [920/1271], Loss: 0.2399\n",
            "Epoch [5/5], Batch [930/1271], Loss: 0.1045\n",
            "Epoch [5/5], Batch [940/1271], Loss: 0.1328\n",
            "Epoch [5/5], Batch [950/1271], Loss: 0.0840\n",
            "Epoch [5/5], Batch [960/1271], Loss: 0.0763\n",
            "Epoch [5/5], Batch [970/1271], Loss: 0.0070\n",
            "Epoch [5/5], Batch [980/1271], Loss: 0.0432\n",
            "Epoch [5/5], Batch [990/1271], Loss: 0.0206\n",
            "Epoch [5/5], Batch [1000/1271], Loss: 0.0078\n",
            "Epoch [5/5], Batch [1010/1271], Loss: 0.6493\n",
            "Epoch [5/5], Batch [1020/1271], Loss: 0.2188\n",
            "Epoch [5/5], Batch [1030/1271], Loss: 0.0114\n",
            "Epoch [5/5], Batch [1040/1271], Loss: 0.5594\n",
            "Epoch [5/5], Batch [1050/1271], Loss: 0.0072\n",
            "Epoch [5/5], Batch [1060/1271], Loss: 0.2722\n",
            "Epoch [5/5], Batch [1070/1271], Loss: 0.2296\n",
            "Epoch [5/5], Batch [1080/1271], Loss: 0.0597\n",
            "Epoch [5/5], Batch [1090/1271], Loss: 0.0293\n",
            "Epoch [5/5], Batch [1100/1271], Loss: 0.1248\n",
            "Epoch [5/5], Batch [1110/1271], Loss: 0.1735\n",
            "Epoch [5/5], Batch [1120/1271], Loss: 0.0279\n",
            "Epoch [5/5], Batch [1130/1271], Loss: 0.0141\n",
            "Epoch [5/5], Batch [1140/1271], Loss: 0.0389\n",
            "Epoch [5/5], Batch [1150/1271], Loss: 0.0470\n",
            "Epoch [5/5], Batch [1160/1271], Loss: 0.0187\n",
            "Epoch [5/5], Batch [1170/1271], Loss: 0.2180\n",
            "Epoch [5/5], Batch [1180/1271], Loss: 0.3640\n",
            "Epoch [5/5], Batch [1190/1271], Loss: 0.0614\n",
            "Epoch [5/5], Batch [1200/1271], Loss: 0.2250\n",
            "Epoch [5/5], Batch [1210/1271], Loss: 0.6911\n",
            "Epoch [5/5], Batch [1220/1271], Loss: 0.1132\n",
            "Epoch [5/5], Batch [1230/1271], Loss: 0.0268\n",
            "Epoch [5/5], Batch [1240/1271], Loss: 0.2732\n",
            "Epoch [5/5], Batch [1250/1271], Loss: 0.0464\n",
            "Epoch [5/5], Batch [1260/1271], Loss: 0.0169\n",
            "Epoch [5/5], Batch [1270/1271], Loss: 0.2982\n",
            "Epoch [5/5] completed. Average Loss: 0.1162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, predicted = torch.max(outputs[0], 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "accuracy = correct / total\n",
        "print('Test Accuracy: {:.2f}'.format(accuracy))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T19:24:43.045331Z",
          "iopub.execute_input": "2023-03-10T19:24:43.046196Z",
          "iopub.status.idle": "2023-03-10T19:26:07.907131Z",
          "shell.execute_reply.started": "2023-03-10T19:24:43.046158Z",
          "shell.execute_reply": "2023-03-10T19:26:07.905767Z"
        },
        "trusted": true,
        "id": "MTR6XJnzq195",
        "outputId": "a2d401fc-07a0-4b91-d84e-a8d3dc3dd5a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predictions for the test set\n",
        "model.eval()\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, predicted = torch.max(outputs[0], 1)\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Get the actual labels for the test set\n",
        "y_true = test_labels\n",
        "\n",
        "# Print the classification report\n",
        "target_names = list(labels_map.keys())\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T19:33:41.381195Z",
          "iopub.execute_input": "2023-03-10T19:33:41.381567Z",
          "iopub.status.idle": "2023-03-10T19:35:06.177644Z",
          "shell.execute_reply.started": "2023-03-10T19:33:41.381532Z",
          "shell.execute_reply": "2023-03-10T19:35:06.176172Z"
        },
        "trusted": true,
        "id": "h3QSlDHqq195",
        "outputId": "e429f8b7-666c-4a11-f51d-0edd395ff624",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "ENTERTAINMENT       0.85      0.89      0.87       974\n",
            "         TECH       0.79      0.67      0.73       418\n",
            "   WORLD NEWS       0.83      0.85      0.84       667\n",
            "     POLITICS       0.79      0.81      0.80      1022\n",
            "     BUSINESS       0.80      0.83      0.81       990\n",
            "       SPORTS       0.92      0.86      0.89      1010\n",
            "\n",
            "     accuracy                           0.84      5081\n",
            "    macro avg       0.83      0.82      0.82      5081\n",
            " weighted avg       0.84      0.84      0.83      5081\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_category(article_text):\n",
        "    # preprocess the article text\n",
        "    preprocessed_text = preprocess_text(article_text)\n",
        "\n",
        "    # tokenize the text\n",
        "    input_ids = tokenizer.encode(preprocessed_text, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "    # make a prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids.to(device), attention_mask=None)\n",
        "        _, predicted = torch.max(outputs[0], 1)\n",
        "\n",
        "    # get the predicted category label\n",
        "    label_map = {v: k for k, v in labels_map.items()}\n",
        "    predicted_label = label_map[predicted.item()]\n",
        "\n",
        "    return predicted_label"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T19:38:23.513043Z",
          "iopub.execute_input": "2023-03-10T19:38:23.514427Z",
          "iopub.status.idle": "2023-03-10T19:38:23.521867Z",
          "shell.execute_reply.started": "2023-03-10T19:38:23.514376Z",
          "shell.execute_reply": "2023-03-10T19:38:23.520798Z"
        },
        "trusted": true,
        "id": "mwyFNj1Hq196"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"Apple unveils new iPhone\"\n",
        "predicted_category = predict_category(article)\n",
        "print(predicted_category)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T20:14:46.769655Z",
          "iopub.execute_input": "2023-03-10T20:14:46.77066Z",
          "iopub.status.idle": "2023-03-10T20:14:46.793137Z",
          "shell.execute_reply.started": "2023-03-10T20:14:46.770614Z",
          "shell.execute_reply": "2023-03-10T20:14:46.792209Z"
        },
        "trusted": true,
        "id": "T6-D_15Dq196",
        "outputId": "3acaf22d-76c5-44c9-f62f-df1d2d14eed7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TECH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"The impact of 5G technology on connectivity and the Internet of Things (IoT).\"\n",
        "predicted_category = predict_category(article)\n",
        "print(predicted_category)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T20:14:48.24765Z",
          "iopub.execute_input": "2023-03-10T20:14:48.248795Z",
          "iopub.status.idle": "2023-03-10T20:14:48.266869Z",
          "shell.execute_reply.started": "2023-03-10T20:14:48.248733Z",
          "shell.execute_reply": "2023-03-10T20:14:48.265745Z"
        },
        "trusted": true,
        "id": "sRQLDyVoq196",
        "outputId": "8bfc7346-f70f-4b9b-98ed-b919b1c8fa63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORLD NEWS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"The latest superhero film, 'Cosmic Avenger,' has become a massive hit, captivating audiences with its thrilling action scenes and special effects. Directed by Emma Roberts, the film features a stellar cast and has received praise for its engaging storyline and innovative visual effects\"\n",
        "predicted_category = predict_category(article)\n",
        "print(predicted_category)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T20:14:49.315652Z",
          "iopub.execute_input": "2023-03-10T20:14:49.316824Z",
          "iopub.status.idle": "2023-03-10T20:14:49.335889Z",
          "shell.execute_reply.started": "2023-03-10T20:14:49.316774Z",
          "shell.execute_reply": "2023-03-10T20:14:49.334506Z"
        },
        "trusted": true,
        "id": "v-b8e_-Qq196",
        "outputId": "d290cce7-7efb-4cf8-f7b3-c6120c050c49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENTERTAINMENT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Directory to save the model\n",
        "save_directory = \"./bert_model\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "if not os.path.exists(save_directory):\n",
        "    os.makedirs(save_directory)\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n"
      ],
      "metadata": {
        "id": "ea5o_FjNq197",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2939277-b908-4d64-c417-8ea990f1d76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./bert_model/tokenizer_config.json',\n",
              " './bert_model/special_tokens_map.json',\n",
              " './bert_model/vocab.txt',\n",
              " './bert_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path in your Google Drive where you want to save the model\n",
        "drive_path = '/content/drive/My Drive/bert_model'\n",
        "\n",
        "# Copy the model directory to Google Drive\n",
        "shutil.copytree(save_directory, drive_path)\n",
        "\n",
        "print(f\"Model saved to {drive_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddoIBHr8Uh7f",
        "outputId": "1096110e-c9c9-4ef8-df1e-08ab4c71790b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Model saved to /content/drive/My Drive/bert_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertForSequenceClassification, AdamW\n",
        "\n",
        "# Load the pre-trained BERT model with a classification head\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_map))\n",
        "model.to(device)\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {avg_epoch_loss:.4f}')\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained('./fine_tuned_bert')\n",
        "tokenizer.save_pretrained('./fine_tuned_bert')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9Dy-i_RVBb5",
        "outputId": "348b975c-39e2-4f95-91c5-d911dd698027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Loss: 0.6810\n",
            "Epoch [2/3], Loss: 0.3867\n",
            "Epoch [3/3], Loss: 0.2425\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine_tuned_bert/tokenizer_config.json',\n",
              " './fine_tuned_bert/special_tokens_map.json',\n",
              " './fine_tuned_bert/vocab.txt',\n",
              " './fine_tuned_bert/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained('./fine_tuned_bert')\n",
        "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_bert')\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quO3YPH3X2nL",
        "outputId": "71cafc5f-e27f-457e-f250-360521b1fc8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = BertForSequenceClassification.from_pretrained('./fine_tuned_bert')\n",
        "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_bert')\n",
        "\n",
        "def predict_category(article_text):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer.encode(article_text, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "    return labels_map_inverse[predicted.item()]\n"
      ],
      "metadata": {
        "id": "9-Bq8snVYPUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Define the directory where you want to save the model\n",
        "drive_path = '/content/drive/MyDrive/bert_fine_tuned'\n",
        "\n",
        "# Load your fine-tuned model and tokenizer (assuming they are already fine-tuned and ready)\n",
        "model = BertForSequenceClassification.from_pretrained('./fine_tuned_bert')\n",
        "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_bert')\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(drive_path)\n",
        "tokenizer.save_pretrained(drive_path)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {drive_path}\")\n"
      ],
      "metadata": {
        "id": "fkxPkLwjfFTb",
        "outputId": "54500691-4b94-4b54-da2d-9ed0e00df85f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer saved to /content/drive/MyDrive/bert_fine_tuned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Prepare DataLoader for test data\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = BertForSequenceClassification.from_pretrained('./fine_tuned_bert')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Lists to store true labels and predictions\n",
        "true_labels = []\n",
        "predictions = []\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "hFY98sVTfFwl",
        "outputId": "8f6a33f1-b928-451c-acda-214ba4d315f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Prepare DataLoader for test data\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = BertForSequenceClassification.from_pretrained('./fine_tuned_bert')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Lists to store true labels and predictions\n",
        "true_labels = []\n",
        "predictions = []\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Generate the classification report\n",
        "target_names = list(labels_map.keys())  # Assuming labels_map is your dictionary for label names\n",
        "report = classification_report(true_labels, predictions, target_names=target_names)\n",
        "\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "7wwFCAuhfilw",
        "outputId": "f87fd557-f42e-4a63-e478-e42735898401",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "ENTERTAINMENT       0.90      0.84      0.87      1480\n",
            "       SPORTS       0.90      0.91      0.91      1485\n",
            "     BUSINESS       0.81      0.80      0.81      1493\n",
            "     POLITICS       0.81      0.85      0.83      1536\n",
            "   WORLD NEWS       0.88      0.85      0.86       991\n",
            "         TECH       0.74      0.82      0.78       636\n",
            "\n",
            "     accuracy                           0.85      7621\n",
            "    macro avg       0.84      0.84      0.84      7621\n",
            " weighted avg       0.85      0.85      0.85      7621\n",
            "\n"
          ]
        }
      ]
    }
  ]
}